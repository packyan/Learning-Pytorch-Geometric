{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置\n",
    "安装jupyter notebook、pytoch、matplotlib等包\n",
    "## 基本数据操作\n",
    "Tensor， 基本和numpy数组差不多，和matlab里一些操作也很像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0286e-38, 9.0919e-39],\n",
       "        [8.9082e-39, 9.2755e-39],\n",
       "        [8.4490e-39, 1.0194e-38],\n",
       "        [9.0919e-39, 8.4490e-39],\n",
       "        [1.0745e-38, 1.0561e-38]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#创建tensor\n",
    "x = torch.empty((5,2))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1221, 0.0389, 0.0697],\n",
      "        [0.8840, 0.0451, 0.5212],\n",
      "        [0.0193, 0.8137, 0.1887],\n",
      "        [0.3860, 0.2646, 0.7590],\n",
      "        [0.2598, 0.3354, 0.8827]])\n"
     ]
    }
   ],
   "source": [
    "#随机建立一个tensor 范围0-1\n",
    "x = torch.rand((5,3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9272, -1.1415, -1.9746, -0.8791],\n",
       "        [-1.0584, -0.5484,  0.0530, -1.1074],\n",
       "        [-1.2382, -1.9759, -0.0450,  1.6494],\n",
       "        [ 0.5545,  0.3283,  0.0760,  0.2881],\n",
       "        [ 0.4399, -0.3384,  0.6106, -0.5153]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#符合标准正态分布的随机建立\n",
    "torch.randn((5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1,10,steps=10)\n",
    "#线性地创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0642,  6.2892, -0.5409,  5.7580], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(torch.arange(1,5, dtype = torch.float64), torch.arange(1,5, dtype = torch.float64))\n",
    "#离散正态分布 这个用途存疑，没遇到过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : tensor([3., 1., 3., 4.]) type : <class 'torch.Tensor'>\n",
      "y : tensor([[0.6175, 0.7324],\n",
      "        [0.0112, 0.2388],\n",
      "        [0.5185, 0.9236],\n",
      "        [0.8491, 0.0439],\n",
      "        [0.7135, 0.6283]], dtype=torch.float64) type : <class 'torch.Tensor'>\n",
      "z : [[0.61754746 0.73241525]\n",
      " [0.01122493 0.23879336]\n",
      " [0.51853918 0.92358194]\n",
      " [0.84911759 0.04394372]\n",
      " [0.7135324  0.62829926]] type : <class 'numpy.ndarray'>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#从列表中建立\n",
    "import numpy as np\n",
    "data = np.random.rand(5,2)\n",
    "x = torch.tensor([3.0,1,3,4])\n",
    "y = torch.from_numpy(data)\n",
    "z = y.numpy()\n",
    "print('x : {} type : {}\\ny : {} type : {}\\nz : {} type : {}\\n '.format(x,type(x),y,type(y),z,type(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.5843, -0.2085, -0.0090],\n",
      "        [-0.1026, -0.5490,  0.0836],\n",
      "        [ 0.8599, -0.1260,  0.3729],\n",
      "        [-0.1219, -0.2635, -0.6295],\n",
      "        [-0.4779, -0.7205,  0.3781]])\n"
     ]
    }
   ],
   "source": [
    "#其他地建立方式\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#获取tensor的形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Tensor的基础操作\n",
    "\n",
    "|               函数                |           功能            |\n",
    "| :-------------------------------: | :-----------------------: |\n",
    "|          Tensor(*sizes)           |       基础构造函数        |\n",
    "|           tensor(data,)           |  类似np.array的构造函数   |\n",
    "|           ones(*sizes)            |         全1Tensor         |\n",
    "|           zeros(*sizes)           |         全0Tensor         |\n",
    "|            eye(*sizes)            |    对角线为1，其他为0     |\n",
    "|          arange(s,e,step          |    从s到e，步长为step     |\n",
    "|        linspace(s,e,steps)        | 从s到e，均匀切分成steps份 |\n",
    "|        rand/randn(*sizes)         |       均匀/标准分布       |\n",
    "| normal(mean,std)/uniform(from,to) |     正态分布/均匀分布     |\n",
    "|            randperm(m)            |         随机排列          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里出现了一个dtype（data type 数据类型嘛）参数，那么pytorch里数据类型有哪些呢？\n",
    "![imgs/datatype.jpg](attachment:imgs/datatype.jpg)\n",
    "\n",
    "## 算术操作\n",
    "加减乘，比较常见"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7977, -0.3612, -0.7910],\n",
       "        [-2.0437, -0.0728,  1.2005],\n",
       "        [ 1.9359,  0.1579, -0.1358],\n",
       "        [-0.2157, -1.1487, -2.5557],\n",
       "        [-1.4501, -0.0460,  1.0075]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(5,3)\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7977, -0.3612, -0.7910],\n",
       "        [-2.0437, -0.0728,  1.2005],\n",
       "        [ 1.9359,  0.1579, -0.1358],\n",
       "        [-0.2157, -1.1487, -2.5557],\n",
       "        [-1.4501, -0.0460,  1.0075]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7977, -0.3612, -0.7910],\n",
      "        [-2.0437, -0.0728,  1.2005],\n",
      "        [ 1.9359,  0.1579, -0.1358],\n",
      "        [-0.2157, -1.1487, -2.5557],\n",
      "        [-1.4501, -0.0460,  1.0075]])\n"
     ]
    }
   ],
   "source": [
    "# inplace 操作 ，就地操作，直接修改y，而不是返回一个新的值\n",
    "# 不需要 y = y.add(x)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引\n",
    "我们还可以使用类似NumPy的索引操作来访问`Tensor`的一部分，需要注意的是：**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。** \n",
    "索引操作在python  里，是直接对对象进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4157,  2.7915,  2.9910],\n",
      "        [-0.1026, -0.5490,  0.0836],\n",
      "        [ 0.8599, -0.1260,  0.3729],\n",
      "        [-0.1219, -0.2635, -0.6295],\n",
      "        [-0.4779, -0.7205,  0.3781]])\n",
      "before : 2.4157216548919678\n",
      "after 100\n",
      "tensor([[ 2.4157,  2.7915,  2.9910],\n",
      "        [-0.1026, -0.5490,  0.0836],\n",
      "        [ 0.8599, -0.1260,  0.3729],\n",
      "        [-0.1219, -0.2635, -0.6295],\n",
      "        [-0.4779, -0.7205,  0.3781]])\n"
     ]
    }
   ],
   "source": [
    "y = x[0,:]\n",
    "y += 1\n",
    "print(x)\n",
    "\n",
    "# 严格说是切片索引是直接操作原始数据，而普通的索引就是简单的 赋值。\n",
    "y = x[0,0]\n",
    "print('before : {}'.format(y))\n",
    "y = 100\n",
    "print('after {}'.format(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|              函数               |                         功能                          |\n",
    "| :-----------------------------: | :---------------------------------------------------: |\n",
    "| index_select(input, dim, index) |      在指定维度dim上选取，比如选取某些行、某些列      |\n",
    "|   masked_select(input, mask)    |       例子如上，a[a>0]，使用ByteTensor进行选取        |\n",
    "|         non_zero(input)         |                     非0元素的下标                     |\n",
    "|    gather(input, dim, index)    | 根据index，在dim维度上选取数据，输出的size与index一样 |\n",
    "\n",
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "mask操作可以这样做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[ 1.1835,  0.4402, -1.7738],\n",
      "        [ 1.2606, -0.8378,  0.0348],\n",
      "        [-0.8016,  0.9382,  0.1095]])\n",
      "tensor([[0, 1, 1],\n",
      "        [0, 1, 1],\n",
      "        [1, 0, 1]], dtype=torch.uint8)\n",
      "tensor([ 0.4402, -1.7738, -0.8378,  0.0348, -0.8016,  0.1095])\n",
      "tensor([ 0.4402, -1.7738, -0.8378,  0.0348, -0.8016,  0.1095])\n",
      "tensor([[0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 1.]])\n",
      "tensor([[0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 1.]])\n",
      "tensor([[ 0.0000,  0.4402, -1.7738],\n",
      "        [ 0.0000, -0.8378,  0.0348],\n",
      "        [-0.8016,  0.0000,  0.1095]])\n",
      "上面三种方法都一样\n",
      "[[0.87520726 0.84689209 0.84645978]\n",
      " [0.43877319 0.22121679 0.77170276]\n",
      " [0.72807338 0.21030772 0.61317052]\n",
      " [0.34889811 0.41063939 0.80024706]\n",
      " [0.82906917 0.43560517 0.4836327 ]]\n",
      "[[False False  True]\n",
      " [False False False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [False  True False]]\n",
      "[0.84645978 0.72807338 0.21030772 0.61317052 0.34889811 0.41063939\n",
      " 0.80024706 0.43560517]\n",
      "[[0.         0.         0.84645978]\n",
      " [0.         0.         0.        ]\n",
      " [0.72807338 0.21030772 0.61317052]\n",
      " [0.34889811 0.41063939 0.80024706]\n",
      " [0.         0.43560517 0.        ]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected backend CPU and dtype Float but got backend CPU and dtype Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-1c8b5616a9a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected backend CPU and dtype Float but got backend CPU and dtype Byte"
     ]
    }
   ],
   "source": [
    "#按照size构造tensor 大写\n",
    "x = torch.Tensor(3,3)\n",
    "print(x)\n",
    "\n",
    "#随机初始一个\n",
    "x = torch.randn(3,3)\n",
    "print(x)\n",
    "print(x < 0.5)\n",
    "\n",
    "#取出符合要求的元素\n",
    "print(x[x<0.5])\n",
    "print(torch.masked_select(x, x < 0.5))\n",
    "\n",
    "#这里需要对类型进行转换\n",
    "mask = x<0.5\n",
    "try:\n",
    "    #直接做会出错，先类型转换一下\n",
    "    print(x * mask)\n",
    "finally:\n",
    "    print(mask.type(torch.float64))\n",
    "    print(mask.float())\n",
    "    print(mask.type_as(x))\n",
    "    print(x*mask.float())\n",
    "    \n",
    "    print('上面三种方法都一样')\n",
    "\n",
    "    w = np.random.rand(5,3)\n",
    "    print(w)\n",
    "    list_mask=np.random.rand(*w.shape) < 0.5\n",
    "    print((list_mask))\n",
    "    #注意两种mask 方式的不一样\n",
    "    print(w[list_mask])\n",
    "    print(w * list_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改变形状\n",
    "用`view()`来改变`Tensor`的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5498,  1.7367, -0.6230],\n",
      "        [ 0.9497, -0.8038,  0.7044],\n",
      "        [-0.1894, -0.4409, -0.7654]])\n",
      "tensor([[-0.5498,  1.7367, -0.6230,  0.9497, -0.8038,  0.7044, -0.1894, -0.4409,\n",
      "         -0.7654]])\n",
      "tensor([[ 6.6600e+02,  1.7367e+00, -6.2303e-01,  9.4971e-01, -8.0376e-01,\n",
      "          7.0442e-01, -1.8935e-01, -4.4093e-01, -7.6543e-01]])\n",
      "tensor([[ 6.6600e+02,  1.7367e+00, -6.2303e-01],\n",
      "        [ 9.4971e-01, -8.0376e-01,  7.0442e-01],\n",
      "        [-1.8935e-01, -4.4093e-01, -7.6543e-01]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "print(x)\n",
    "y = x.view(-1,9)\n",
    "print(y)\n",
    "y[0][0] = 666\n",
    "print(y)\n",
    "print(x)\n",
    "print(x.view(y.shape) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意`view()`返回的新tensor与源tensor共享内存（其实是同一个tensor），也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度)**\n",
    "\n",
    "使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.6600e+02,  1.7367e+00, -6.2303e-01,  9.4971e-01, -8.0376e-01,\n",
      "          7.0442e-01, -1.8935e-01, -4.4093e-01, -7.6543e-01]])\n",
      "tensor([[ 8.8800e+02,  1.7367e+00, -6.2303e-01,  9.4971e-01, -8.0376e-01,\n",
      "          7.0442e-01, -1.8935e-01, -4.4093e-01, -7.6543e-01]])\n",
      "tensor([[ 8.8800e+02,  1.7367e+00, -6.2303e-01],\n",
      "        [ 9.4971e-01, -8.0376e-01,  7.0442e-01],\n",
      "        [-1.8935e-01, -4.4093e-01, -7.6543e-01]])\n",
      "tensor([[ 0.0558,  0.2770, -0.3539],\n",
      "        [-1.0539, -0.9913, -0.0664],\n",
      "        [-0.4048,  1.1260,  0.1387]])\n",
      "tensor([[ 7.7700e+02,  2.7697e-01, -3.5393e-01, -1.0539e+00, -9.9126e-01,\n",
      "         -6.6355e-02, -4.0477e-01,  1.1260e+00,  1.3871e-01]])\n",
      "tensor([[ 0.0558,  0.2770, -0.3539],\n",
      "        [-1.0539, -0.9913, -0.0664],\n",
      "        [-0.4048,  1.1260,  0.1387]])\n"
     ]
    }
   ],
   "source": [
    "y2 = x.reshape(-1,9)\n",
    "#但是此函数并不能保证返回的是其拷贝\n",
    "#不推荐使用reshape函数\n",
    "print(y2)\n",
    "y2[0][0] = 888\n",
    "print(y2)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(3,3)\n",
    "print(x)\n",
    "#clone() 返回副本 然后再view\n",
    "y3 = x.clone().view(-1,9)\n",
    "y3[0][0] = 777\n",
    "print(y3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0626])\n",
      "0.06256353855133057\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数运算\n",
    "PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。\n",
    "PyTorch中的`Tensor`支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。\n",
    "\n",
    "|               函数               |               功能                |\n",
    "| :------------------------------: | :-------------------------------: |\n",
    "|              trace               |     对角线元素之和(矩阵的迹)      |\n",
    "|               diag               |            对角线元素             |\n",
    "|            triu/tril             | 矩阵的上三角/下三角，可指定偏移量 |\n",
    "|              mm/bmm              |     矩阵乘法，batch的矩阵乘法     |\n",
    "| addmm/addbmm/addmv/addr/badbmm.. |             矩阵运算              |\n",
    "|                t                 |               转置                |\n",
    "|            dot/cross             |             内积/外积             |\n",
    "|             inverse              |             求逆矩阵              |\n",
    "|               svd                |            奇异值分解             |\n",
    "\n",
    "##  广播机制 Broadcast\n",
    "前面我们看到如何对两个形状相同的`Tensor`做按元素运算。当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5271,  0.8905,  0.9801],\n",
      "        [-2.2593,  1.1630, -2.1686],\n",
      "        [-2.1433, -1.8415, -0.1482],\n",
      "        [-0.7257,  0.1922, -1.0287],\n",
      "        [-0.5688,  0.5115, -3.8828]])\n",
      "tensor([[1],\n",
      "        [2]]) tensor([[1, 2, 3]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-3cfbc7eba8bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,3)\n",
    "y = torch.randn(3)\n",
    "\n",
    "#注意， 要维度相同呢\n",
    "print(x.add(y))\n",
    "\n",
    "#或者双方都有一个维度为1：\n",
    "x = torch.arange(1,3).view(2,1)\n",
    "y = torch.arange(1,4).view(1,3)\n",
    "print(x,y)\n",
    "print(y.add(x))\n",
    "\n",
    "#inplace 操作不会触发 broadcast操作\n",
    "\n",
    "#不同会怎么样\n",
    "x = torch.randn(5,3)\n",
    "y = torch.randn(4)\n",
    "x.add(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 运算的内存开销\n",
    "\n",
    "前面说了，索引、`view`是不会开辟新内存的，而像`y = x + y`这样的运算是会新开内存的，然后将`y`指向新内存。为了演示这一点，我们可以使用Python自带的`id`函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False \n",
    "```\n",
    "\n",
    "如果想指定结果到原来的`y`的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把`x + y`的结果通过`[:]`写进`y`对应的内存中。\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True\n",
    "```\n",
    "\n",
    "我们还可以使用运算符全名函数中的`out`参数或者自加运算符`+=`(也即`add_()`)达到上述效果，例如`torch.add(x, y, out=y)`和`y += x`(`y.add_(x)`\n",
    "\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 `Tensor`和NumPy相互转换\n",
    "\n",
    "我们很容易用`numpy()`和`from_numpy()`将`Tensor`和NumPy中的数组相互转换。但是需要注意的一点是：\n",
    "**这两个函数所产生的的`Tensor`和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！**\n",
    "\n",
    "> 还有一个常用的将NumPy中的array转换成`Tensor`的方法就是`torch.tensor()`, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的`Tensor`和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94481227  0.08396565  0.48694674]\n",
      " [-0.80725734  0.51887894 -2.26874429]\n",
      " [-0.07192007  0.0992277  -0.05335085]]\n",
      "tensor([[ 1.0000,  1.0000,  1.0000],\n",
      "        [-0.8073,  0.5189, -2.2687],\n",
      "        [-0.0719,  0.0992, -0.0534]], dtype=torch.float64) [[ 1.          1.          1.        ]\n",
      " [-0.80725734  0.51887894 -2.26874429]\n",
      " [-0.07192007  0.0992277  -0.05335085]]\n",
      "tensor([[ 0.4332,  0.0149, -0.3476],\n",
      "        [-1.1909, -0.7243,  2.1325],\n",
      "        [-0.4085,  0.3653, -0.2600]])\n",
      "[[222.         222.         222.        ]\n",
      " [ -1.1908967   -0.7243283    2.1325471 ]\n",
      " [ -0.40848246   0.3652671   -0.26003286]] tensor([[222.0000, 222.0000, 222.0000],\n",
      "        [ -1.1909,  -0.7243,   2.1325],\n",
      "        [ -0.4085,   0.3653,  -0.2600]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(3,3)\n",
    "print(x)\n",
    "x_tensor = torch.from_numpy(x)\n",
    "x_tensor[0] = 1\n",
    "print(x_tensor, x)\n",
    "\n",
    "x = torch.randn(3,3)\n",
    "print(x)\n",
    "x_np = x.numpy()\n",
    "x_np[0] = 222\n",
    "print(x_np, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特别提醒[注意Tensor大小写]\n",
    "\n",
    "最重要的区别t.Tensor和t.tensor：不论输入的类型是什么，`t.tensor()`都会进行数据拷贝，不会共享内存；`t.Tensor()`与`Numpy`共享内存，但当Numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。\n",
    "可使用`t.from_numpy()` 或者 `t.detach()` 将Numpy转为Tensor，与原Numpy数据共享内存。\n",
    "附上实验证明\n",
    "常规转换：使用`t.from_numpy()` 将Numpy转为Tensor，使用 `torch.numpy()` 将Tensor转为Numpy\n",
    "这里两种方式都是共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有在CPU上的`Tensor`（除了`CharTensor`）都支持与NumPy数组相互转换。\n",
    "\n",
    "此外上面提到还有一个常用的方法就是直接用`torch.tensor()`将NumPy数组转换成`Tensor`，需要注意的是该方法总是会进行数据拷贝，返回的`Tensor`和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34780906  0.32796017  1.58553259]\n",
      " [-0.0211887   0.41080688 -0.99407921]\n",
      " [ 0.53120997 -0.5750506   0.7851364 ]]\n",
      "tensor([[ 2.2200e+02,  2.2200e+02,  2.2200e+02],\n",
      "        [-2.1189e-02,  4.1081e-01, -9.9408e-01],\n",
      "        [ 5.3121e-01, -5.7505e-01,  7.8514e-01]], dtype=torch.float64) [[-0.34780906  0.32796017  1.58553259]\n",
      " [-0.0211887   0.41080688 -0.99407921]\n",
      " [ 0.53120997 -0.5750506   0.7851364 ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3,3)\n",
    "print(x)\n",
    "x_np = torch.tensor(x)\n",
    "x_np[0] = 222\n",
    "print(x_np, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 `Tensor` on GPU\n",
    "\n",
    "用方法`to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2606,  1.4849, -1.2709,  0.4529,  1.6387],\n",
      "        [ 0.3824,  1.8151, -0.0333, -0.8842,  2.2383],\n",
      "        [ 0.9259,  0.8722, -0.0529,  0.6273, -0.1627],\n",
      "        [ 3.8030,  1.4720,  1.3928,  0.3691,  1.4324],\n",
      "        [ 0.7160,  3.1162,  0.8997,  1.8662,  1.2300]], device='cuda:0')\n",
      "tensor([[ 2.2606,  1.4849, -1.2709,  0.4529,  1.6387],\n",
      "        [ 0.3824,  1.8151, -0.0333, -0.8842,  2.2383],\n",
      "        [ 0.9259,  0.8722, -0.0529,  0.6273, -0.1627],\n",
      "        [ 3.8030,  1.4720,  1.3928,  0.3691,  1.4324],\n",
      "        [ 0.7160,  3.1162,  0.8997,  1.8662,  1.2300]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "x = torch.randn(5,5)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device = device)  # 直接创建一个在GPU上的Tensor\n",
    "\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
